{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import tiktoken\n",
    "import fitz\n",
    "import shutil, random, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo-1106\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages, temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert letter ruling context PDF to text\n",
    "pdf_to_convert = fitz.open(\"/Users/st414/Documents/PLR/plr_literature.pdf\")\n",
    "letterRuling_context = \"\"\n",
    "for page in pdf_to_convert:\n",
    "    text = page.get_text()\n",
    "    letterRuling_context += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to iterate through multiple PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a specific year's PDFs\n",
    "# assign directory\n",
    "# directory = \"/Users/st414/Documents/PLR/sample_plrs\"\n",
    "\n",
    "\n",
    "# iterate over files in that directory\n",
    "def files_by_year(year, folder_path):\n",
    "    plr_list_by_year = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        # print(file)\n",
    "        if len(file) == 11:\n",
    "            if str(year)[-2:] == str(file)[0:2]:\n",
    "                plr_list_by_year.append(file)\n",
    "        if len(file) == 13 and \"_\" in file:\n",
    "            if str(year)[-2:] == str(file)[0:2]:\n",
    "                plr_list_by_year.append(file)\n",
    "        if len(file) == 13:\n",
    "            if str(year) == str(file)[0:4]:\n",
    "                plr_list_by_year.append(file)\n",
    "        if len(file) == 15 and \"_\" in file:\n",
    "            if str(year) == str(file)[0:4]:\n",
    "                plr_list_by_year.append(file)\n",
    "    # print(file_list_by_year)\n",
    "    return plr_list_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to iterate through multiple PLRs and get classification\n",
    "# def get_plr_classification(year, folder_path):\n",
    "#     plr_classification_list = []\n",
    "#     plr_list_by_year = files_by_year(year, folder_path)\n",
    "#     for plr in plr_list_by_year:\n",
    "#         print(plr)\n",
    "#         plr_classification_dict = {}\n",
    "#         plr_filepath = os.path.join(folder_path, plr)\n",
    "#         pdf_to_convert = fitz.open(plr_filepath)\n",
    "#         plr_text = \"\"\n",
    "#         for page in pdf_to_convert:\n",
    "#             text = page.get_text()\n",
    "#             plr_text += text\n",
    "#         # get classification\n",
    "#         prompt = f\"\"\"\n",
    "#         Your task is to classify letter rulings as adverse or non-adverse by using\n",
    "#         knowledge and context from the literature provided to you below, delimited\n",
    "#         by triple dollar signs.\n",
    "\n",
    "#         Literature: $$${letterRuling_context}$$$\n",
    "\n",
    "#         Below is the letter ruling, delimited by triple backticks, which has to be classified as Adverse or Non Adverse.\n",
    "\n",
    "#         Letter Ruling: ```{plr_text}```\n",
    "\n",
    "#         Provide your output as one of the two values: Adverse or Non-Adverse.\n",
    "#         \"\"\"\n",
    "\n",
    "#         response = get_completion(prompt)\n",
    "#         plr_classification_dict = {int(plr.split(\".\")[0]): response}\n",
    "#         plr_classification_list.append(plr_classification_dict)\n",
    "#     return plr_classification_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate through multiple PLRs and get classification\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "def get_plr_classification(year, folder_path):\n",
    "    plr_classification_list = []\n",
    "    plr_list_by_year = files_by_year(year, folder_path)\n",
    "    for plr in plr_list_by_year:\n",
    "        plr_classification_dict = {}\n",
    "        plr_filepath = os.path.join(folder_path, plr)\n",
    "        pdf_to_convert = fitz.open(plr_filepath)\n",
    "        plr_text = \"\"\n",
    "        for page in pdf_to_convert:\n",
    "            text = page.get_text()\n",
    "            plr_text += text\n",
    "        # get classification\n",
    "        if len(encoding.encode(plr_text)) > 13000:\n",
    "            pass\n",
    "        else:\n",
    "            print(plr)\n",
    "            prompt = f\"\"\"\n",
    "        Your task is to classify letter rulings as adverse or non-adverse by using\n",
    "        knowledge and context from the literature provided to you below, delimited\n",
    "        by triple dollar signs.\n",
    "\n",
    "        Literature: $$${letterRuling_context}$$$\n",
    "\n",
    "        Below is the letter ruling, delimited by triple backticks, which has to be classified as Adverse or Non Adverse.\n",
    "\n",
    "        Letter Ruling: ```{plr_text}```\n",
    "\n",
    "        Provide your output as one of the two values: Adverse or Non-Adverse.\n",
    "        \"\"\"\n",
    "\n",
    "        response = get_completion(prompt)\n",
    "        plr_classification_dict = {int(plr.split(\".\")[0]): response}\n",
    "        plr_classification_list.append(plr_classification_dict)\n",
    "    return plr_classification_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and clean Tagged PLRs csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_plrs = pd.read_excel(\n",
    "    \"/Users/st414/Documents/PLR/tagged_plrs.xlsx\",\n",
    "    sheet_name=\"Sheet1\",\n",
    "    converters={\n",
    "        \"Adverse, Discretionary\": int,\n",
    "        \"Partially Adverse\": int,\n",
    "        \"Not Adverse\": int,\n",
    "        \"Mandatory\": int,\n",
    "        \"Employee not IC\": int,\n",
    "        \"Revocation\": int,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the DataFrame using melt\n",
    "tagged_plrs_melted = pd.melt(\n",
    "    tagged_plrs, var_name=\"classification\", value_name=\"plr_number\"\n",
    ")\n",
    "\n",
    "# Remove rows with NA values\n",
    "tagged_plrs_melted = tagged_plrs_melted.dropna(subset=[\"plr_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>plr_number</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200644027</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200014009</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200837002</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Adverse, Discretionary</td>\n",
       "      <td>200804004</td>\n",
       "      <td>Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200825052</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>Mandatory</td>\n",
       "      <td>202036007</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>199940034</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>Mandatory</td>\n",
       "      <td>201718040</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>201940010</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200241045</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              classification plr_number          tag\n",
       "1714             Not Adverse  200644027  Non-Adverse\n",
       "1391             Not Adverse  200014009  Non-Adverse\n",
       "1796             Not Adverse  200837002  Non-Adverse\n",
       "34    Adverse, Discretionary  200804004      Adverse\n",
       "1786             Not Adverse  200825052  Non-Adverse\n",
       "2242               Mandatory  202036007        Other\n",
       "1370             Not Adverse  199940034  Non-Adverse\n",
       "2202               Mandatory  201718040        Other\n",
       "1985             Not Adverse  201940010  Non-Adverse\n",
       "1540             Not Adverse  200241045  Non-Adverse"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column for adverse, non-adverse\n",
    "# Creating a new column 'Tag' based on a condition\n",
    "test_set = tagged_plrs_melted.copy()\n",
    "test_set['tag'] = 'Other' # Initialize the 'Tag' column with 'Other' as the default value\n",
    "\n",
    "# Using DataFrame.loc[] to set values based on the condition\n",
    "test_set.loc[test_set['classification'] == 'Adverse, Discretionary', 'tag'] = 'Adverse'\n",
    "test_set.loc[test_set['classification'] == 'Not Adverse', 'tag'] = 'Non-Adverse'\n",
    "# tagged_plrs_melted.loc[tagged_plrs_melted['Classification'] == 'Partially Adverse', 'Tag'] = 'Partially-Adverse'\n",
    "\n",
    "# Displaying the modified dataframe\n",
    "test_set.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>plr_number</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>201330006</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>201415015</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200953030</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200431009</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200631025</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200237026</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200442003</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>201019028</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>201230033</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>Not Adverse</td>\n",
       "      <td>200718018</td>\n",
       "      <td>Non-Adverse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     classification plr_number          tag\n",
       "1932    Not Adverse  201330006  Non-Adverse\n",
       "1952    Not Adverse  201415015  Non-Adverse\n",
       "1841    Not Adverse  200953030  Non-Adverse\n",
       "1626    Not Adverse  200431009  Non-Adverse\n",
       "1705    Not Adverse  200631025  Non-Adverse\n",
       "1536    Not Adverse  200237026  Non-Adverse\n",
       "1636    Not Adverse  200442003  Non-Adverse\n",
       "1849    Not Adverse  201019028  Non-Adverse\n",
       "1910    Not Adverse  201230033  Non-Adverse\n",
       "1751    Not Adverse  200718018  Non-Adverse"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove other tags from Tagged dataset and keep only adverse and non-adverse\n",
    "reference_set = test_set[test_set['tag'] != \"Other\"]\n",
    "reference_set.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate tagged PLRs from Elisa's PLR library for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the pdfs with _ or - in them\n",
    "# Function to check if a filename contains \"_\" or \"-\"\n",
    "def has_underscore_or_dash(filename):\n",
    "    return \"_\" in filename or \"-\" in filename\n",
    "\n",
    "# Source and destination directories\n",
    "source_folder = \"/Users/st414/Documents/PLR/elisa_plrs/files_definite_plr\"\n",
    "destination_folder = \"/Users/st414/Documents/PLR/elisa_plrs/duplicates_and_errors\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Iterate through files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Check if it's a PDF file\n",
    "        source_file_path = os.path.join(source_folder, filename)\n",
    "        if has_underscore_or_dash(filename):\n",
    "            # Move the file to the destination folder\n",
    "            destination_file_path = os.path.join(destination_folder, filename)\n",
    "            shutil.move(source_file_path, destination_file_path)\n",
    "            print(f\"Moved {filename} to {destination_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're left with 28,135 PLRs after removing duplicates and error files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your source folder containing files\n",
    "source_folder = \"/Users/st414/Documents/PLR/elisa_plrs/files_definite_plr\"\n",
    "\n",
    "# Path to your destination folder where you want to copy the files\n",
    "destination_folder_train = \"/Users/st414/Documents/PLR/elisa_plrs/train_set\"\n",
    "\n",
    "# Iterate through the file numbers in the DataFrame\n",
    "for file_number in reference_set['PLR_Number']:\n",
    "    source_file_path_org = os.path.join(source_folder, str(file_number) + \".pdf\")\n",
    "    # print(source_file_path_org)\n",
    "    source_file_path_ = os.path.join(source_folder, str(file_number)[2:] + \".pdf\")\n",
    "    # print(source_file_path_)\n",
    "\n",
    "    # Check if the file exists in the source folder\n",
    "    if os.path.exists(source_file_path_org):\n",
    "        # Copy the file to the destination folder\n",
    "        destination_file_path = os.path.join(destination_folder_train, str(file_number) + \".pdf\")\n",
    "        shutil.copy(source_file_path_org, destination_file_path)\n",
    "        print(f\"File '{file_number}' copied to '{destination_folder_train}'\")\n",
    "\n",
    "    elif os.path.exists(source_file_path_):\n",
    "        destination_file_path = os.path.join(destination_folder_train, str(file_number) + \".pdf\")\n",
    "        shutil.copy(source_file_path_, destination_file_path)\n",
    "        print(f\"File '{file_number}' copied to '{destination_folder_train}'\")\n",
    "print(\"File copying completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate accuracy, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_df(list_of_dicts):\n",
    "    keys = []\n",
    "    values = []\n",
    "\n",
    "    for dict in list_of_dicts:\n",
    "        for key, value in dict.items():\n",
    "            keys.append(key)\n",
    "            values.append(value)\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame({'plr_number': keys, 'tag': values})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(list_of_dicts, reference_set):\n",
    "    # Convert list of dicts to DataFrame\n",
    "    list_df = list_to_df(list_of_dicts)\n",
    "\n",
    "    # Merge the DataFrame and list_df based on the PLR number\n",
    "    merged_df = pd.merge(reference_set, list_df, on='plr_number', suffixes=('_ref', '_list'))\n",
    "    #print(merged_df.loc[(merged_df['tag_ref'] == 'Adverse') & (merged_df['tag_list'] == 'Non-Adverse')])\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(merged_df['tag_ref'] == merged_df['tag_list']) / len(merged_df)\n",
    "\n",
    "    # Calculate recall\n",
    "    true_positives = sum((merged_df['tag_ref'] == 'Adverse') & (merged_df['tag_list'] == 'Adverse'))\n",
    "    false_negatives = sum((merged_df['tag_ref'] == 'Adverse') & (merged_df['tag_list'] != 'Adverse'))\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    # Calculate precision\n",
    "    false_positives = sum((merged_df['tag_ref'] != 'Adverse') & (merged_df['tag_list'] == 'Adverse'))\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Recall: {recall*100:.2f}%\")\n",
    "    print(f\"Precision: {precision*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through multiple years\n",
    "def iterate_multiple_years(years, folder_path):\n",
    "    years_list = []\n",
    "    for y in years:\n",
    "        years_list.extend(get_plr_classification(y, folder_path))\n",
    "    return years_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics from 2015 to 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201552032.pdf\n",
      "201521009.pdf\n",
      "201528002.pdf\n",
      "201551006.pdf\n",
      "201543004.pdf\n",
      "201503001.pdf\n",
      "201532026.pdf\n",
      "201549019.pdf\n",
      "201538026.pdf\n",
      "201625001.pdf\n",
      "201623001.pdf\n",
      "201619007.pdf\n",
      "201610006.pdf\n",
      "201633021.pdf\n",
      "201628005.pdf\n",
      "201628004.pdf\n",
      "201628006.pdf\n",
      "201622007.pdf\n",
      "201616002.pdf\n",
      "201748005.pdf\n",
      "201740016.pdf\n",
      "201740005.pdf\n",
      "201722014.pdf\n",
      "201751011.pdf\n",
      "201706006.pdf\n",
      "201741012.pdf\n",
      "201722010.pdf\n",
      "201816004.pdf\n",
      "201828010.pdf\n",
      "201815005.pdf\n",
      "201819006.pdf\n",
      "201825003.pdf\n",
      "201811002.pdf\n",
      "201825006.pdf\n",
      "201951001.pdf\n",
      "201926006.pdf\n",
      "201926007.pdf\n",
      "201927005.pdf\n",
      "201927012.pdf\n",
      "201943020.pdf\n",
      "202005020.pdf\n",
      "202016001.pdf\n",
      "202014004.pdf\n",
      "202022005.pdf\n",
      "202014005.pdf\n",
      "202014001.pdf\n",
      "202014002.pdf\n",
      "202014003.pdf\n",
      "202138001.pdf\n",
      "202114001.pdf\n",
      "202125007.pdf\n",
      "202144005.pdf\n",
      "202118021.pdf\n"
     ]
    }
   ],
   "source": [
    "years = list(range(2015,2022))\n",
    "folder_path = \"/Users/st414/Documents/PLR/elisa_plrs/train_set\"\n",
    "l=iterate_multiple_years(years, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            classification plr_number  tag_ref     tag_list\n",
      "5   Adverse, Discretionary  201610006  Adverse  Non-Adverse\n",
      "8   Adverse, Discretionary  201622007  Adverse  Non-Adverse\n",
      "17  Adverse, Discretionary  201741012  Adverse  Non-Adverse\n",
      "24  Adverse, Discretionary  201943020  Adverse  Non-Adverse\n",
      "Accuracy: 77.78%\n",
      "Recall: 87.50%\n",
      "Precision: 82.35%\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(l, reference_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics from 2000 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2000,2023))\n",
    "folder_path = \"/Users/st414/Documents/PLR/elisa_plrs/train_set\"\n",
    "l=iterate_multiple_years(years, folder_path)\n",
    "calculate_metrics(l, reference_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.64%\n",
      "Recall: 87.50%\n",
      "Precision: 60.00%\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(l, reference_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
