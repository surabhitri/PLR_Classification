{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install --upgrade pip\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import fitz\n",
    "import shutil, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo-1106\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages, temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which can be achieved through longer prompts that offer more clarity and context.\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Building code skeleton and prompt experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert letter ruling context PDF to text\n",
    "pdf_to_convert = fitz.open(\"/Users/st414/Documents/PLR/plr_literature.pdf\")\n",
    "letterRuling_context = \"\"\n",
    "for page in pdf_to_convert:\n",
    "    text = page.get_text()\n",
    "    letterRuling_context += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PLR PDF to text\n",
    "pdf_to_convert = fitz.open(\"/Users/st414/Documents/PLR/test_plrs/PLR 9428031.pdf\")\n",
    "plr_9428031 = \"\"\n",
    "for page in pdf_to_convert:\n",
    "    text = page.get_text()\n",
    "    plr_9428031 += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"LetterRuling\": \"Non-Adverse\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# get classification\n",
    "prompt = f\"\"\"\n",
    "Your task is to classify letter rulings as adverse or non-adverse by using\n",
    "knowledge and context from the literature provided to you below, delimited\n",
    "by triple dollar signs.\n",
    "\n",
    "Literature: $$${letterRuling_context}$$$\n",
    "\n",
    "Classify the letter ruling below, delimited by triple \n",
    "backticks, in JSON format as Adverse or Non-Adverse.\n",
    "\n",
    "Letter Ruling: ```{plr_9428031}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PLR PDF to text\n",
    "pdf_to_convert = fitz.open(\"/Users/st414/Documents/PLR/test_plrs/PLR 200224023.pdf\")\n",
    "plr_200224023 = \"\"\n",
    "for page in pdf_to_convert:\n",
    "    text = page.get_text()\n",
    "    plr_200224023 += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"LetterRuling\": \"Adverse\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get classification\n",
    "prompt = f\"\"\"\n",
    "Your task is to classify letter rulings as adverse or non-adverse by using\n",
    "knowledge and context from the literature provided to you below, delimited\n",
    "by triple dollar signs.\n",
    "\n",
    "Literature: $$${letterRuling_context}$$$\n",
    "\n",
    "Classify the letter ruling below, delimited by triple \n",
    "backticks, in JSON format as Adverse or Non-Adverse.\n",
    "\n",
    "Letter Ruling: ```{plr_200224023}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
